<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <title>æ–‡æ¡£</title>
  </head>
  <body>
  <h1 align="center">XGBoost-Bagging é›†æˆå­¦ä¹ å™¨åˆ¤æ–­ç™Œç—‡ç»†èƒæ€§è´¨</h1>
    <h2 align="center">è®¡ç®—æœºç§‘å­¦ä¸æŠ€æœ¯å­¦é™¢&nbsp; PB20051150&nbsp; é™†å­ç¦ <br>
    </h2>
    <h2>1. ç§‘å­¦æŠ€æœ¯åŸç†</h2>
	<p>æœ¬ç¨‹åºä¸»è¦å®ç°çš„æ˜¯ä¸€ä¸ªæœºå™¨å­¦ä¹ ä¸­çš„ä¸€ä¸ªè‡ªå·±è®¾è®¡æ­å»ºçš„é›†æˆå­¦ä¹ å™¨ã€‚é€šè¿‡å¯¹åŸºå­¦ä¹ å™¨XGBoostè¿›è¡ŒBaggingï¼Œæ¥å®ç°ä¸€ä¸ªåœ¨å°æ ·æœ¬ä¸Šæ³›åŒ–æ€§èƒ½è¾ƒå¥½ï¼Œä¸æ˜“è¿‡æ‹Ÿåˆï¼Œå‡†ç¡®æ€§è¾ƒé«˜ï¼Œä¸”è®­ç»ƒæ—¶é—´å’Œå¼€é”€è¾ƒå°çš„é«˜æ•ˆçš„å­¦ä¹ å™¨ã€‚è¿™ä¸ªå­¦ä¹ å™¨è¢«åº”ç”¨äºé€šè¿‡å¯¹ä¸€ç³»åˆ—ç™Œç»†èƒçš„æ•°æ®ï¼ŒåŒ…æ‹¬ç™Œç»†èƒçš„radiusã€textureã€areaç­‰ï¼Œä»¥åŠå¯¹åº”çš„labelï¼Œå³æ˜¯å¦ä¸ºæ¶æ€§çš„ï¼Œè¿›è¡Œå­¦ä¹ ï¼Œä»è€Œå¾—åˆ°ä¸€ä¸ªmodelï¼Œå¯ä»¥ç”¨æ¥åˆ¤æ–­ä¸€ä¸ªæœªçŸ¥çš„æ ·æœ¬æœ‰æ›´å¤§å¯èƒ½æ˜¯æ¶æ€§è¿˜æ˜¯è‰¯æ€§ã€‚</p>
	<h3>1.1 Bagging</h3>
  <p>é›†æˆå­¦ä¹ (ensemble learning)é€šè¿‡æ„å»ºå¹¶ç»“åˆå¤šä¸ªå­¦ä¹ å™¨æ¥æå‡æ€§èƒ½, Baggingæ˜¯é›†æˆå­¦ä¹ çš„ä¸€ç§ï¼Œæ˜¯ä¸€ç§ä¸ªä½“å­¦ä¹ å™¨ä¹‹é—´ä¸å­˜åœ¨å¼ºä¾èµ–å…³ç³»ï¼Œå¯ä»¥åŒæ—¶ç”Ÿæˆçš„å¹¶è¡ŒåŒ–æ–¹æ³•ã€‚</p>
	  
	 
  <p>é›†æˆå­¦ä¹ å¯ä»¥è·å¾—æ¯”å•ä¸€å­¦ä¹ å™¨æ˜¾è‘—ä¼˜è¶Šçš„æ³›åŒ–æ€§èƒ½ï¼Œè¿™ä¸€ç‚¹å¯¹å¼±å­¦ä¹ å™¨å°¤ä¸ºæ˜æ˜¾ã€‚å¯ä»¥ç›´è§‚çš„ç†è§£ä¸ºé›†æˆå­¦ä¹ å™¨æ˜¯ç”±å¾ˆå¤šåŸºå­¦ä¹ å™¨å¾—å‡ºçš„ç»“æœè¿›è¡ŒæŠ•ç¥¨äº§ç”Ÿçš„ï¼Œå¦‚æœæ¯ä¸ªåŸºå­¦ä¹ å™¨è‡ªèº«éƒ½æœ‰è¾ƒå¤§çš„å¯èƒ½åˆ¤æ–­æ­£ç¡®ï¼Œè€Œå®ƒä»¬åˆ¤æ–­çš„ç»“æœåˆç›¸äº’ç‹¬ç«‹ï¼Œé‚£ä¹ˆå®ƒä»¬æœ€åæŠ•ç¥¨ä¹‹åæ­£ç¡®ç»“æœå å¤šæ•°çš„å¯èƒ½æ€§å°±å¾ˆå¤§ï¼Œäºæ˜¯æœ€åæŠ•ç¥¨çš„æ­£ç¡®ç‡ä¹Ÿä¼šå¾ˆå¤§ã€‚
  </p>
  <p>å¯ä»¥å°±è¿™ä¸€ç‚¹è¿›è¡Œç®€å•çš„åˆ†æã€‚è€ƒè™‘äºŒåˆ†ç±»é—®é¢˜ï¼Œå‡è®¾åŸºåˆ†ç±»å™¨çš„é”™è¯¯ç‡ä¸ºï¼š</p>
<div style="text-align: center">  <img src="images/b1.png"  height="50" style="margin: 0 atuo"/></div>
  <p>å‡è®¾é›†æˆé€šè¿‡ç®€å•æŠ•ç¥¨æ³•ç»“åˆğ‘‡ä¸ªåˆ†ç±»å™¨</p>
<div style="text-align: center">  <img src="images/b2.png"  height="80" style="margin: 0 atuo"/></div>
<p>å‡è®¾åŸºåˆ†ç±»å™¨çš„é”™è¯¯ç‡ç›¸äº’ç‹¬ç«‹ï¼Œä»¤ğ‘›(ğ‘‡)è¡¨ç¤ºğ‘‡ä¸ªåŸºåˆ†ç±»å™¨ä¸­å¯¹ğ’™é¢„æµ‹æ­£ç¡®çš„ä¸ªæ•°, è‹¥æœ‰è¶…è¿‡åŠæ•°çš„åŸºåˆ†ç±»å™¨æ­£ç¡®åˆ™åˆ†ç±»å°±æ­£ç¡®, åˆ™æœ‰ï¼š
</p>
<div style="text-align: center">  <img src="images/b3.png"  height="80" style="margin: 0 atuo"/></div>
<p>åˆ©ç”¨Hoeffdingä¸ç­‰å¼ï¼š</p>
<div style="text-align: center">  <img src="images/b4.png"  height="55" style="margin: 0 atuo"/></div>
<p>å¯ä»¥å¾—åˆ°ä»¥ä¸‹æ¨å¯¼ï¼š</p>
 <div style="text-align: center">  <img src="images/b5.png"  height="45" style="margin: 0 atuo"/></div>
<p>å¦<img src="images/b6.png"  height="30" style="margin: 10 0 0 0 atuoï¼›"/></p>
<div style="text-align: center">  <img src="images/b8.png"  height="75" style="margin: 0 atuo"/></div>
<p>ä»ä¸Šé¢çš„æ¨å¯¼å¯ä»¥çœ‹å‡ºï¼Œå½“åŸºå­¦ä¹ å™¨ä¹‹é—´åˆ¤æ–­ç›¸äº’ç‹¬ç«‹ï¼Œä¸”å‡†ç¡®æ€§è¾ƒé«˜æ—¶ï¼Œéšç€BaggingåŸºåˆ†ç±»å™¨æ•°ç›®çš„å¢åŠ ï¼Œé›†æˆçš„é”™è¯¯ç‡å°†æŒ‡æ•°çº§ä¸‹é™ï¼Œæœ€ç»ˆè¶‹å‘äº0ã€‚å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œé›†æˆçš„é”™è¯¯ç‡ä¸‹é™æ˜¯æœ‰æ¡ä»¶çš„ï¼Œå³åŸºå­¦ä¹ å™¨ä¹‹é—´è¦å°½é‡åšåˆ°ç‹¬ç«‹ï¼Œè€Œä¸”åŸºå­¦ä¹ å™¨çš„å‡†ç¡®æ€§ä¹Ÿè¦è¾ƒå¥½ï¼ˆè‡³å°‘æ˜æ˜¾å¥½äºççŒœï¼‰ï¼Œä¹Ÿå°±æ˜¯è¯´éœ€è¦å¥½è€Œä¸åŒï¼Œè¿™å°±å–å†³äºåŸºå­¦ä¹ å™¨çš„é€‰å–å’Œæ„å»ºäº†ã€‚è¿™ä¸€ç‚¹åœ¨åé¢å¯¹åŸºå­¦ä¹ å™¨XGBoostçš„ä»‹ç»ä¸­ä¼šæœ‰è¿›ä¸€æ­¥è¯´æ˜ã€‚</p>
<p>Baggingçš„ç®—æ³•å¦‚ä¸‹ï¼š</p>
 <div>  <img src="images/c1.png"  height="250" style="margin: 0 atuo"/></div>
<p>ä»ä¸­å¯ä»¥çœ‹å‡ºï¼Œç®—æ³•æ˜¯é€šè¿‡ç›´æ¥å¯¹åŸºå­¦ä¹ å™¨çš„ç»“æœè¿›è¡Œç®€å•æŠ•ç¥¨ï¼Œå–å‡ºæœ€å¤§çš„ä½œä¸ºç»“æœã€‚æˆ‘å¯¹è¿™ä¸€ç‚¹åšå‡ºäº†æ”¹è¿›ï¼Œå³ä¸å†ä½¿ç”¨ç®€å•æŠ•ç¥¨ï¼Œè€Œæ˜¯æŠŠæ¯ä¸ªæ ·æœ¬åœ¨å„ä¸ªåŸºå­¦ä¹ å™¨ä¸Šçš„ç»“æœä½œä¸ºæ–°çš„è®­ç»ƒæ•°æ®ï¼Œè¾“å…¥åˆ°ä¸€ä¸ªLogistic Regressionå­¦ä¹ å™¨ä¸­ï¼Œä»è€Œå¾—å‡ºæœ€åçš„ç»“æœã€‚ä¸‹é¢æ˜¯æˆ‘çš„ç¨‹åºçš„æ¡†å›¾ã€‚</p> 
<div style="text-align: center">  <img src="images/d1.png"  height="250" style="margin: 0 atuo"/></div>
<p>Logistic Regressionå³å¯¹ç‡å›å½’æ¨¡å‹ï¼Œæœ¬è´¨ä¸Šæ˜¯ä¸€ç§çº¿æ€§æ¨¡å‹ã€‚å¯¹ç‡å›å½’çš„ä¼˜åŒ–ç›®æ ‡å‡½æ•°å¦‚ä¸‹ï¼š</p>
<div style="text-align: center">  <img src="images/e1.png"  height="70" style="margin: 0 atuo"/></div>
<p>æœ¬è´¨ä¸Šå°±æ˜¯æ‹Ÿåˆï¼š</p>
<div style="text-align: center">  <img src="images/e2.png"  height="35" style="margin: 0 atuo"/></div>
<p>å¯ä»¥çœ‹å‡ºç›¸å½“äºå­¦ä¹ å¾—åˆ°æœ€å¥½çš„åŠ æƒæ±‚å’Œï¼Œåœ¨è¿™é‡Œå°±è¡¨ç°ä¸ºé€šè¿‡å­¦ä¹ å¾—åˆ°æœ€ä¼˜çš„åŠ æƒæŠ•ç¥¨çš„æƒå€¼wã€‚è¿™ä¸ªæƒé‡çš„ç›´è§‚æ„ä¹‰å°±æ˜¯æ¯ä¸ªåŸºå­¦ä¹ å™¨çš„åˆ¤æ–­èƒ½åŠ›ä¸åŒï¼Œå¯¹äºæœ€åç»“æœçš„è´¡çŒ®ä¹Ÿä¸åŒï¼Œæƒé‡å°±ä»£è¡¨åŸºå­¦ä¹ å™¨å¯¹äºæœ€åçš„ç»“æœçš„è´¡çŒ®ã€‚è¿™æ ·å¯ä»¥æé«˜é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</p>
<h3>1.2 XGBoost</h3>
<p>XGBoostæ˜¯é™ˆå¤©å¥‡äº2015å¹´XGBoost: A Scalable Tree Boosting Systemè®ºæ–‡ä¸­æå‡ºçš„ä¸€ç§Boostingçš„å­¦ä¹ æ–¹æ³•ï¼Œè¿™ç§æ–¹æ³•æ˜¯å¯¹æ¢¯åº¦æå‡ç®—æ³•çš„æ”¹è¿›ï¼Œæ±‚è§£æŸå¤±å‡½æ•°æå€¼æ—¶ä½¿ç”¨äº†ç‰›é¡¿æ³•ï¼Œå°†æŸå¤±å‡½æ•°æ³°å‹’å±•å¼€åˆ°äºŒé˜¶ï¼Œå¦å¤–æŸå¤±å‡½æ•°ä¸­åŠ å…¥äº†æ­£åˆ™åŒ–é¡¹ï¼Œæ˜¯ä¸€ç§è¡¨ç°éå¸¸ä¼˜å¼‚çš„ç®—æ³•ã€‚å…¶å®˜æ–¹çš„åº“åº•å±‚ç”±C++å®ç°ï¼Œè€Œæˆ‘ä½¿ç”¨Pythonçš„ç§‘å­¦è®¡ç®—åº“numpyå¯¹è¯¥ç®—æ³•è¿›è¡Œäº†å¤ç°ã€‚</p>
	  <p>XGBoostXGBoostæ˜¯ç”±kä¸ªåŸºæ¨¡å‹ç»„æˆçš„ä¸€ä¸ªåŠ æ³•æ¨¡å‹ï¼Œå‡è®¾æˆ‘ä»¬ç¬¬tæ¬¡è¿­ä»£è¦è®­ç»ƒçš„æ ‘æ¨¡å‹æ˜¯f<font size="2">t</font>(x)ï¼Œåˆ™æœ‰ï¼š</p>
<div style="text-align: center">  <img src="images/f1.png"  height="110" style="margin: 0 atuo"/></div>
<p>åœ¨å­¦ä¹ ç¬¬tä¸ªåŸºæ¨¡å‹æ—¶ï¼ŒXGBoost è¦ä¼˜åŒ–çš„ç›®æ ‡å‡½æ•°ä¸ºï¼š</p>
<div style="text-align: center">  <img src="images/f2.png"  height="190" style="margin: 0 atuo"/></div>
<p>ç”±äºæœ¬æ¬¡åˆ¤æ–­ç™Œç»†èƒçš„æ€§è´¨çš„é—®é¢˜æ˜¯ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ï¼Œæ‰€ä»¥losså‡½æ•°æ˜¯ï¼š</p>
<div style="text-align: center">  <img src="images/f3.png"  height="40" style="margin: 0 atuo"/></div>
<p>æ³°å‹’å±•å¼€ï¼Œå¾—åˆ°ï¼š</p>
<div style="text-align: center">  <img src="images/f4.png"  height="45" style="margin: 0 atuo"/></div>
<p>å…¶ä¸­ï¼Œ</p>
<div style="text-align: center">  <img src="images/f5.png"  height="70" style="margin: 0 atuo"/></div>
<p>æ­¤æ—¶çš„ä¼˜åŒ–ç›®æ ‡å˜ä¸ºï¼š</p>
<div style="text-align: center">  <img src="images/f6.png"  height="50" style="margin: 0 atuo"/></div>
<p>å»æ‰å¯¹äºä¼˜åŒ–æ— å…³çš„å¸¸é‡ä¹‹åï¼Œå¾—åˆ°ï¼š</p>
<div style="text-align: center">  <img src="images/f7.png"  height="60" style="margin: 0 atuo"/></div>
<p>æˆ‘çš„XGBoostçš„åŸºå­¦ä¹ å™¨æ˜¯å†³ç­–æ ‘ï¼Œæ‰€ä»¥è¿˜è¦æ¨å¯¼å‡ºå†³ç­–æ ‘çš„ç®—æ³•ã€‚</p>
<p>æˆ‘ä»¬å¯¹äºæŸä¸€æ£µå†³ç­–æ ‘ï¼Œä»–çš„æƒ©ç½šä¸ºï¼š</p>
<div style="text-align: center">  <img src="images/f8.png"  height="55" style="margin: 0 atuo"/></div>
<p>æ‰€ä»¥ï¼Œæˆ‘ä»¬åœ¨æ ‘çš„ç»“æ„ç¡®å®šæ—¶ï¼Œå¯ä»¥åšå¦‚ä¸‹ä¼˜åŒ–ï¼š</p>
<div style="text-align: center">  <img src="images/f9.png"  height="190" style="margin: 0 atuo"/></div>
<p>ç®€è®°ï¼š</p>
<div style="text-align: center">  <img src="images/f11.png"  height="40" style="margin: 0 atuo"/></div>
<p>äºæ˜¯æœ‰ï¼š</p>
<div style="text-align: center">  <img src="images/f10.png"  height="60" style="margin: 0 atuo"/></div>
<p>å¯ä»¥æ¨å‡ºæ ‘çš„æœ€ä¼˜å¶èŠ‚ç‚¹å€¼ä¸ºï¼š</p>
<div style="text-align: center">  <img src="images/f12.png"  height="50" style="margin: 0 atuo"/></div>
<p>å‡è®¾æ¨¡å‹åœ¨æŸä¸€èŠ‚ç‚¹å®Œæˆç‰¹å¾åˆ†è£‚ï¼Œåˆ†è£‚å‰çš„ç›®æ ‡å‡½æ•°å¯ä»¥å†™ä¸ºï¼š</p>
<div style="text-align: center">  <img src="images/f13.png"  height="55" style="margin: 0 atuo"/></div>
<p>åˆ†è£‚åçš„ç›®æ ‡å‡½æ•°ä¸ºï¼š</p>
<div style="text-align: center">  <img src="images/f14.png"  height="50" style="margin: 0 atuo"/></div>
<p>åˆ™å¯¹äºç›®æ ‡å‡½æ•°æ¥è¯´ï¼Œåˆ†è£‚åçš„æ”¶ç›Šä¸ºï¼š</p>
<div style="text-align: center">  <img src="images/f15.png"  height="60" style="margin: 0 atuo"/></div>
<p>ä»è€Œå¾—å‡ºå†³ç­–æ ‘èŠ‚ç‚¹åˆ†è£‚çš„ä¼ªä»£ç ä¸ºï¼š</p>
<div style="">  <img src="images/g1.png"  height="400" style="margin: 0 atuo"/></div>
<p>XGBoostçš„æ¨¡å‹å¯¹äºæ•°æ®çš„åå·®éå¸¸å°ï¼Œå¯ä»¥é«˜åº¦æ‹Ÿåˆæ•°æ®ï¼Œä½†å¦‚æœä»…ä»…è®­ç»ƒå¤šä¸ªXGBoostæ¨¡å‹å‚ä¸Baggingä¸å¯é¿å…åœ°ä¼šè®©XGBoostæ¨¡å‹ä¹‹é—´ç›¸ä¼¼æ€§è¾ƒé«˜ï¼Œå¹¶ä¸ç‹¬ç«‹ï¼Œä»è€Œæ— æ³•å‘æŒ¥Baggingçš„ä¼˜åŠ¿ã€‚ä¸ºäº†è®©æ¯ä¸ªXGBooståŸºå­¦ä¹ å™¨ä¹‹é—´æœ‰å·®å¼‚è€Œä¸”ä¸è¿‡åˆ†å½±å“åŸºå­¦ä¹ å™¨æ€§èƒ½ï¼Œæˆ‘è®¾è®¡äº†éšæœºå±æ€§é‡‡æ ·çš„æ–¹æ³•ï¼Œå³åœ¨æ¯ä¸ªXGBoostå­¦ä¹ å™¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œå…ˆç”¨randomçš„æ–¹æ³•éšæœºé€‰å–æ ·æœ¬å±æ€§ä¸­çš„å‡ ä¸ªï¼Œç„¶ååªé’ˆå¯¹è¿™å‡ ä¸ªå±æ€§å¯¹XGBoostè¿›è¡Œè®­ç»ƒï¼Œä»è€Œè®©XGBooståŸºå­¦ä¹ å™¨è¾¾åˆ°å¥½è€Œä¸åŒçš„æ•ˆæœã€‚è¿™ä¹Ÿæ˜¯æˆ‘é’ˆå¯¹ç™Œç»†èƒå±æ€§è¾ƒå¤šè¿™ä¸ªç‰¹ç‚¹åšå‡ºçš„é€‰æ‹©ã€‚</p>
<h3>1.3 åˆ¤æ–­ç™Œç»†èƒæ€§è´¨</h3>
<p>è®¡ç®—æœºæŠ€æœ¯ç‰¹åˆ«æ˜¯æœºå™¨å­¦ä¹ çš„å‘å±•ç»™è®¸å¤šé¢†åŸŸå¸¦æ¥äº†æ–°çš„å¯èƒ½ï¼Œè€Œæœºå™¨å­¦ä¹ åœ¨åŒ»ç–—é¢†åŸŸçš„åº”ç”¨ç»™ç–¾ç—…çš„è¯Šæ–­å¸¦æ¥äº†å¾ˆå¤§çš„ä¾¿æ·ã€‚å¯¹äºå¤§é‡çš„æ•°æ®ï¼Œäººå·¥åˆ¤æ–­å¾ˆå¯èƒ½å¤±è¯¯ï¼Œè€Œä¸”æ•ˆç‡ä½ä¸‹ï¼Œè€Œé€šè¿‡æœºå™¨å­¦ä¹ åˆ©ç”¨ä¹‹å‰çš„æ ·æœ¬è®­ç»ƒå‡ºå‡†ç¡®æ€§è¾ƒé«˜çš„æ¨¡å‹ä¹‹åï¼Œå°±å¯ä»¥åœ¨ä¸€å®šç¨‹åº¦ä¸Šè¾…åŠ©è¿™ç§è¯Šæ–­çš„è¿‡ç¨‹ã€‚</p>
<p>ç”±äºéƒ¨åˆ†ç–¾ç—…çš„æ ·æœ¬æ•°æ®æ˜¯æœ‰é™çš„ï¼Œè®­ç»ƒæ•°æ®é‡å¯èƒ½è¾ƒå°ï¼Œè€Œå¯¹äºå°æ ·æœ¬è®­ç»ƒå¾ˆå¯èƒ½æŠŠæ ·æœ¬è‡ªèº«çš„å±æ€§å½“ä½œæ³›åŒ–çš„ç‰¹å¾å­¦ä¹ åˆ°æ¨¡å‹ä¸­ï¼Œä»è€Œå¯¼è‡´è¿‡æ‹Ÿåˆã€‚æœ¬æ¬¡æˆ‘å®ç°çš„XGBoost_Baggingæ˜¯å¯¹äºé€‚åº”å°æ ·æœ¬çš„æ¨¡å‹çš„ä¸€ä¸ªæ¢ç´¢ï¼Œåˆ›é€ æ€§åœ°æŠŠBaggingå’ŒBoostingä¸¤ç§é›†æˆå­¦ä¹ æ–¹æ³•ç»“åˆèµ·æ¥ï¼Œç»¼åˆäº†Boostingé™ä½åå·®å’ŒBaggingé™ä½æ–¹å·®çš„ç‰¹ç‚¹ï¼Œåœ¨æœ‰é™çš„æ•°æ®é›†ä¸Šè·å¾—äº†è¾ƒé«˜çš„å‡†ç¡®æ€§ï¼Œè¯æ˜äº†è¿™ç§æ–¹å¼çš„æ½œåŠ›ï¼Œåœ¨ç™Œç—‡è¯Šæ–­ä»¥åŠå…¶ä»–å°æ ·æœ¬ã€ç²¾åº¦è¦æ±‚é«˜çš„åº”ç”¨ä¸­æœ‰è¾ƒå¤§çš„ç°å®æ„ä¹‰ã€‚</p>

<h2>2. è®¾è®¡æ–¹æ¡ˆ</h2>
    <h2>2.1 æ•´ä½“æ–‡ä»¶ç»„ç»‡ç»“æ„</h2>
	  <p>ç¨‹åºçš„ç›®å½•å’Œæ–‡ä»¶ç»„ç»‡å½¢å¼å¦‚ä¸‹ã€‚</p>
	 <div style="">  <img src="images/h1.png"  height="400" style="margin: 0 atuo"/></div>
	å…¶ä¸­main.pyæ˜¯æœ€é¡¶å±‚çš„ä¸»ç¨‹åºï¼Œåªè¦è¿è¡Œè¿™ä¸ªæ–‡ä»¶å°±å¯ä»¥å®Œæˆæ¨¡å‹è®­ç»ƒå’Œæµ‹è¯•ã€‚ä¸»è¦æœ‰4ä¸ªpackageï¼ˆBaggingã€datasetã€Logisticã€XGBoostï¼‰å’Œ4ä¸ªépackageæ–‡ä»¶å¤¹ï¼ˆimagesã€modelã€resultsã€dataï¼‰ã€‚dataseté‡Œé¢æœ‰Canserç±»ï¼Œç”¨äºå¤„ç†wpbc.dataæ–‡ä»¶ä¸­çš„canseræ•°æ®é›†ï¼Œå¹¶è¿”å›éœ€è¦çš„ç±»å‹çš„æ•°æ®ã€‚Baggingè¿™ä¸ªpackageé‡Œé¢å­˜æ”¾çš„ä¸»è¦æ˜¯Baggingç±»ï¼Œåœ¨è®­ç»ƒåŸºå­¦ä¹ å™¨æ—¶è°ƒç”¨äº†XGBoostè¿™ä¸ªpackageä¸­çš„XGBoostç±»ï¼Œåœ¨æŠ•ç¥¨æ—¶è°ƒç”¨äº†Logisticè¿™ä¸ªpackageä¸­çš„LogisticRegressionç±»ã€‚imagesä¸­å­˜æ”¾ç¨‹åºç»˜åˆ¶çš„ä¸€äº›å›¾åƒï¼Œmodelå­˜æ”¾è®­ç»ƒå¾—å‡ºçš„æ¨¡å‹ï¼Œresultså­˜æ”¾ç¨‹åºçš„ä¸€äº›è¾“å‡ºï¼Œdataå­˜æ”¾æ•°æ®æ–‡ä»¶ã€‚
    <h2>2.2 Bagging<br>
    </h2>
	  <p>Baggingç±»å¦‚ä¸‹</p>
	  <pre><code>class Bagging(object):

    def __init__(self, n_estimator=10, attributeRate=0.5, train=True, model_path=None, sample_list_path=None,
                 logistic_path=None, w_train=True):
        # number of base estimators
        self.n_estimator = n_estimator
        # to randomly select attributes
        self.sample_lists = []
        # models of base estimators
        self.models = []
        # logistic estimator used to predict result with result from base estimators
        self.logistic = None
        # the rate of attributes selected randomly
        self.attributeRate = attributeRate
        # whether train base estimators or use saved model
        self.train = train
        # whether the voting model need to be trained
        self.w_train = w_train
        # paths to model of base estimators, sample list, and voting model
        self.model_path = model_path
        self.sample_list_path = sample_list_path
        self.logistic_path = logistic_path

        # get models
        if self.train is False and os.path.exists(self.model_path) and os.path.exists(self.sample_list_path):
            with open(self.model_path, 'rb') as file:
                self.models = pickle.load(file)
            with open(self.sample_list_path, 'rb') as file:
                self.sample_lists = pickle.load(file)
        if self.w_train is False and os.path.exists(self.logistic_path):
            with open(self.logistic_path, 'rb') as file:
                self.logistic = pickle.load(file)

    def fit(self, X, y, rate):
        """train model"""

        # split train set (some for base estimator, some for voting model)
        l = X.shape[0]
        l1 = int(l * rate)  # rate is the percentage of data used in training base estimators
        X0 = X[l1:]
        y0 = y[l1:]
        X = X[:l1]
        y = y[:l1]

        if self.train is True:

            for i in range(self.n_estimator):
                # randomly select some attributes (attributeRate of total)
                sample_num = int(self.attributeRate * X.shape[1])
                sample_list = [i for i in range(X.shape[1])]
                sample_list = random.sample(sample_list, sample_num)
                print(sample_list)
                self.sample_lists.append(sample_list)

                # train models of base estimator
                X1 = X[:, sample_list]
                model = XGBoost(n_trees=20, depth=5, min_impurity=1e-7, tol=1, lam=30)
                model.fit(X1, y)
                self.models.append(model)

            # save model
            modelsFile = "../model/{}_{}_models.pkl".format(self.n_estimator, int(self.attributeRate * 10))
            sampleListsFile = "../model/{}_{}_sampleLists.pkl".format(self.n_estimator, int(self.attributeRate * 10))

            with open(modelsFile, "wb") as file:
                pickle.dump(self.models, file)
            with open(sampleListsFile, "wb") as file:
                pickle.dump(self.sample_lists, file)

        if self.w_train is True:
            # train voting model
            self.logistic = self.fit_w(X0, y0)
            logisticFile = "../model/{}_{}_logistic.pkl".format(self.n_estimator, int(self.attributeRate * 10))
            with open(logisticFile, "wb") as file:
                pickle.dump(self.logistic, file)

    def fit_w(self, X, y):
        """train voting model"""
        y1 = []
        # make the result of n base estimators into new attributes
        for i in range(self.n_estimator):
            sample_list = self.sample_lists[i]
            X1 = X[:, sample_list]
            model = self.models[i]
            y_pred = model.predict(X1)
            y1.append(y_pred)
        y1 = np.array(y1)
        print("y1_shape:", y1.shape)
        y1 = y1.transpose()
        print("y1_shape:", y1.shape)
        # voting model is logistic regression
        model = LogisticRegression()
        model.fit(y1, y)
        return model

    def predict(self, X):
        """predict result"""
        y1 = []
        print("sample_lists_size:", len(self.sample_lists))
        print("model_size:", len(self.models))

        # make the result of n base estimators into new attributes
        for i in range(self.n_estimator):
            sample_list = self.sample_lists[i]
            X1 = X[:, sample_list]
            model = self.models[i]
            y_pred = model.predict(X1)
            print(y_pred)

            y1.append(y_pred)

        y1 = np.array(y1)
        print("y1_shape:", y1.shape)
        y1 = y1.transpose()
        print("y1_shape:", y1.shape)
        y = self.logistic.predict(y1)

        return y</code></pre>
	  <p>å…¶ä¸­fitå‡½æ•°æ˜¯ç”¨æ¥è®­ç»ƒçš„ï¼Œfit_wæ˜¯ç”¨æ¥è®­ç»ƒæŠ•ç¥¨æƒé‡çš„ï¼Œpredictç”¨æ¥æµ‹è¯•ã€‚trainç”¨æ¥æ ‡è®°æ˜¯å¦è®­ç»ƒï¼Œå¦‚æœTrueåˆ™è®­ç»ƒï¼Œå¦‚æœä¸ºFalseåˆ™ä½¿ç”¨å·²æœ‰æ¨¡å‹ã€‚train_wå¯¹åº”wçš„è®­ç»ƒï¼ŒåŒç†</p>
	<h3>XGBoost</h3>
	  <pre><code>import numpy as np


def divide_on_feature(X, feature_i, threshold):
    """divide by feature threshold"""
    X_1 = np.array([sample for sample in X if sample[feature_i] <= threshold])
    X_2 = np.array([sample for sample in X if sample[feature_i] > threshold])

    return X_1, X_2


class XGBoostTreeNode:

    def __init__(self, feature_i=None, threshold=None,
                 value=None, right_branch=None, left_branch=None):
        self.feature_i = feature_i
        self.threshold = threshold
        self.value = value
        self.right_branch = right_branch
        self.left_branch = left_branch


class Sigmoid:
    """sigmoid function class"""
    def __call__(self, x):
        return 1 / (1 + np.exp(-x))

    def gradient(self, x):
        return self.__call__(x) * (1 - self.__call__(x))


class LogisticLoss:
    """logistic loss"""
    def __init__(self):
        sigmoid = Sigmoid()
        self._func = sigmoid
        self._grad = sigmoid.gradient

    def loss(self, y, y_pred):
        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)
        p = self._func(y_pred)
        return y * np.log(p) + (1 - y) * np.log(1 - p)

    def gradient(self, y, y_pred):
        p = self._func(y_pred)
        return -(y - p)

    def hess(self, y, y_pred):
        p = self._func(y_pred)
        return p * (1 - p)


class XGBoostRegressionTree(object):
    """XGBoost Regression Tree"""
    def __init__(self, min_samples_split=2, min_impurity=1e-7,
                 tree_depth=float("inf"), loss=None, lam=None):
        self.root = None
        self.min_samples_split = min_samples_split
        self.min_impurity = min_impurity
        self.tree_depth = tree_depth
        self.loss = loss
        self.lam = lam

    def split_y(self, y):
        """split y"""
        col = int(np.shape(y)[1] / 2)
        y, y_pred = y[:, :col], y[:, col:]
        return y, y_pred

    def gain(self, y, y_pred):
        """gain of a split"""
        nominator = np.power((self.loss.gradient(y, y_pred)).sum(), 2)
        denominator = self.loss.hess(y, y_pred).sum()
        return 0.5 * (nominator / denominator + self.lam)

    def gain_by_taylor(self, y, y1, y2):
        """total gain"""
        y, y_pred = self.split_y(y)
        y1, y1_pred = self.split_y(y1)
        y2, y2_pred = self.split_y(y2)

        true_gain = self.gain(y1, y1_pred)
        false_gain = self.gain(y2, y2_pred)
        gain = self.gain(y, y_pred)
        return true_gain + false_gain - gain

    def approximate_update(self, y):
        """produce leaf value"""
        y, y_pred = self.split_y(y)
        gradient = np.sum(self.loss.gradient(y, y_pred), axis=0)
        hessian = np.sum(self.loss.hess(y, y_pred), axis=0)
        update_approximation = - gradient / (hessian + self.lam)
        return update_approximation

    def fit(self, X, y):
        """train model"""
        self.root = self._build_tree(X, y)
        self.loss = None

    def _build_tree(self, X, y, current_depth=0):
        """build tree"""
        largest_impurity = 0
        best_criteria = None
        best_sets = None

        Xy = np.concatenate((X, y), axis=1)

        n_samples, n_features = np.shape(X)

        if n_samples >= self.min_samples_split and current_depth <= self.tree_depth:
            for feature_i in range(n_features):
                feature_values = np.expand_dims(X[:, feature_i], axis=1)
                unique_values = np.unique(feature_values)
                unique_values.sort()

                for i in range(len(unique_values) - 1):
                    threshold = (unique_values[i] + unique_values[i + 1]) / 2

                    Xy1, Xy2 = divide_on_feature(Xy, feature_i, threshold)

                    if len(Xy1) > 0 and len(Xy2) > 0:
                        y1 = Xy1[:, n_features:]
                        y2 = Xy2[:, n_features:]

                        impurity = self.gain_by_taylor(y, y1, y2)

                        if impurity > largest_impurity:
                            largest_impurity = impurity
                            best_criteria = {"feature_i": feature_i, "threshold": threshold}
                            best_sets = {
                                "leftX": Xy1[:, :n_features],
                                "lefty": Xy1[:, n_features:],
                                "rightX": Xy2[:, :n_features],
                                "righty": Xy2[:, n_features:]
                            }
        # print("impurity:", largest_impurity)
        if largest_impurity > self.min_impurity:
            right_branch = self._build_tree(best_sets["rightX"], best_sets["righty"], current_depth + 1)
            left_branch = self._build_tree(best_sets["leftX"], best_sets["lefty"], current_depth + 1)
            return XGBoostTreeNode(feature_i=best_criteria["feature_i"], threshold=best_criteria[
                "threshold"], right_branch=right_branch, left_branch=left_branch)

        leaf_value = self.approximate_update(y)
        return XGBoostTreeNode(value=leaf_value)

    def predict_value(self, x, tree=None):

        if tree is None:
            tree = self.root

        if tree.value is not None:
            return tree.value

        feature_value = x[tree.feature_i]

        if feature_value <= tree.threshold:
            branch = tree.left_branch
        else:
            branch = tree.right_branch

        return self.predict_value(x, branch)

    def predict(self, X):
        y_pred = []
        for x in X:
            y_pred.append(self.predict_value(x))
        return y_pred


class XGBoost(object):
    """XGBoost class"""
    def __init__(self, n_trees=300, tol=0.01, min_split_n=2,
                 min_impurity=1e-7, depth=2, lam=1):
        self.n_trees = n_trees
        self.tol = tol
        self.min_split_n = min_split_n
        self.min_impurity = min_impurity
        self.depth = depth
        self.loss = LogisticLoss()
        self.trees = []
        self.lam=lam

        for _ in range(n_trees):
            tree = XGBoostRegressionTree(
                min_samples_split=self.min_split_n,
                min_impurity=min_impurity,
                tree_depth=self.depth,
                loss=self.loss,
                lam=self.lam
            )

            self.trees.append(tree)

    def fit(self, X, y):
        """train model"""
        m = X.shape[0]
        y = np.reshape(y, (m, -1))
        y_pred = np.zeros(np.shape(y))
        losses = []
        for i in range(self.n_trees):
            tree = self.trees[i]
            y_and_pred = np.concatenate((y, y_pred), axis=1)
            tree.fit(X, y_and_pred)
            update_pred = tree.predict(X)
            update_pred = np.reshape(update_pred, (m, -1))
            y_pred += update_pred * self.tol
            loss = np.sum(self.loss.loss(y, y_pred))
            losses.append(loss)
        print(losses)

    def predict(self, X):
        """predict test set"""
        y_pred = None
        m = X.shape[0]
        for tree in self.trees:
            update_pred = tree.predict(X)
            update_pred = np.reshape(update_pred, (m, -1))
            if y_pred is None:
                y_pred = np.zeros_like(update_pred)
            y_pred += self.tol * update_pred

        y_pred = y_pred.flatten()
        y_pred[y_pred < 0.5] = 0
        y_pred[y_pred > 0.5] = 1

        return y_pred
</code></pre>
<p>å…¶ä¸­XGBoostç±»è°ƒç”¨XGBoostTreeNodeç±»ä½œä¸ºåŸºå­¦ä¹ å™¨ï¼ŒLogisticLossç±»ç”¨äºæä¾›lossçš„ä¸€é˜¶å’ŒäºŒé˜¶å¯¼çš„è®¡ç®—ã€‚</p>
<h3>2.3 Logistic Regression</h3>
<pre><code>import numpy as np


class LogisticRegression:

    def __init__(self, penalty="l2"):
        self.w = None
        err_msg = "penalty must be 'l1' or 'l2', but got: {}".format(penalty)
        assert penalty in ["l2", "l1"], err_msg

    def sigmoid(self, x):
        """The logistic sigmoid function"""
        return 1 / (1 + np.exp(-x))

    def fit(self, X, y, tol=1e-4, max_iter=1000):
        """
        Fit the regression coefficients via gradient descent or other methods
        """
        self.w = np.ones_like(X[0])
        loss_list = []
        for i in range(int(max_iter)):
            w_d = 0
            loss = 0
            for j in range(y.shape[0]):
                w_d = w_d + (X[j] * (y[j] - self.sigmoid(np.dot(self.w, X[j]))))
                if i % 100 == 0:
                    loss = loss - y[j] * np.dot(self.w, X[j]) + np.log(1 + np.exp(np.dot(self.w, X[j])))
            if i % 100 == 0:
                loss_list.append(loss)
            self.w = self.w + tol * w_d

        return loss_list

    def predict(self, X):
        """
        Use the trained model to generate prediction probabilities on a new
        collection of data points.
        """
        y_predict = np.zeros(X.shape[0])
        y_predict[self.sigmoid((X * self.w).sum(axis=1)).reshape(1, -1).squeeze() > 0.5] = 1
        return y_predict
</code></pre>
	<p>ç”¨äºæŠ•ç¥¨æƒé‡çš„å­¦ä¹ ï¼Œfitä¸ºè®­ç»ƒå‡½æ•°ï¼Œpredictä¸ºé¢„æµ‹å‡½æ•°ã€‚</p>
	<h3>2.4 Canser</h3>
	<pre><code>import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


class Canser(object):
    def __init__(self, path='../data/cancer/wpbc.data'):
        """read the dataset and delete useless attributes"""
        self.path = path
        df = pd.read_csv(path, sep=',', names=['name{}'.format(i) for i in range(41)])
        df.drop('name0', axis=1, inplace=True)  # drop the ID
        df.drop('name34', axis=1, inplace=True)
        df.drop('name33', axis=1, inplace=True)
        df.drop('name35', axis=1, inplace=True)
        df.drop('name36', axis=1, inplace=True)
        df.drop('name37', axis=1, inplace=True)
        df.drop('name38', axis=1, inplace=True)
        df.drop('name39', axis=1, inplace=True)
        df.drop('name40', axis=1, inplace=True)
        df.name1=df.name1.map({'N': 1, 'R': 0})
        df.info()
        data = np.array(df).astype(np.float32)
        self.X = data[:, 1:]
        self.y = data[:, 0].astype(np.int)
        print("X shape:", self.X.shape)
        print("y shape:", self.y.shape)
        print("first 20 X's:")
        print(self.X[:20])
        print("first 20 y's:")
        print(self.y[:20])
        self.plot_features()

    def getXy(self):
        """return X y"""
        return self.X, self.y

    def plot_features(self):
        X0 = self.X[self.y == 0]
        y0 = np.where(self.y == 0)[0]
        X1 = self.X[self.y == 1]
        y1 = np.where(self.y == 1)[0]
        plt.figure(figsize=(40, 20))
        plt.title("distribution of different attributes")
        for i in range(self.X.shape[1]):
            x0 = X0[:, i]
            x1 = X1[:, i]
            print("x0 shape:", x0.shape)
            print("y0 shape:", y0.shape)
            print("y1 shape:", y1.shape)
            print("x1 shape:", x1.shape)
            plt.subplot(4, 8, i + 1)
            plt.scatter(y0, x0, color="r", label="recurrent")
            plt.scatter(y1, x1, color="b", label="nonrecurrent")
            plt.xlabel("instances")
            plt.ylabel("value")
            plt.title("attribute {}".format(i + 1))
            # plt.show()

        plt.savefig("../images/attributes.png")
		
</code></pre>
<p>è¿›è¡Œæ•°æ®çš„å¤„ç†ï¼Œé€šè¿‡pandasä»æ–‡ä»¶ä¸­è¯»å‡ºæ•°æ®ï¼Œå†ç­›å»ä¸€äº›æ— ç”¨çš„å±æ€§ï¼ˆå¦‚ç¼–å·ï¼‰ï¼Œå¹¶è¿”å›Xï¼Œyï¼ŒXæ˜¯dataï¼Œyæ˜¯labelã€‚plot_featureså›¾ç”¨äºæ•°æ®å¯è§†åŒ–ã€‚</p>
    <h2>3. åˆ›æ–°æ€§æè¿°</h2>
    <p>ç¨‹åºä¸­æ‰€æœ‰ä»£ç éƒ½æ˜¯æˆ‘è‡ªå·±å®Œæˆï¼Œæœºå™¨å­¦ä¹ éƒ¨åˆ†åªè°ƒç”¨äº†numpyæ¥å®ç°ï¼Œå„ä¸ªæ–‡ä»¶ä¸­ä»£ç æ€»å…±500å¤šè¡Œï¼Œå·¥ä½œé‡è¿˜æ˜¯è¾ƒå¤§çš„ã€‚<br>
	<p>å…¶ä¸­ä¸»è¦çš„åˆ›æ–°çš„æœ‰ï¼š</p>
	<ul>
	<li>XGBoostå’ŒBaggingçš„ç»“åˆã€‚ç»“åˆäº†ä¸¤ç§é›†æˆå­¦ä¹ æ–¹æ³•ï¼šå¹¶è¡Œçš„Baggingå’Œä¸²è¡Œçš„Boostingã€‚å……åˆ†å‘æŒ¥äº†Boostingå‡å°åå·®ï¼ŒBaggingå‡å°æ–¹å·®çš„ä¼˜ç‚¹ï¼Œæå‡äº†ç¨‹åºçš„å‡†ç¡®æ€§å’Œæ³›åŒ–æ€§èƒ½ã€‚</li>
	<li>Baggingç»“åˆåŸºå­¦ä¹ å™¨çš„æ—¶å€™é€šè¿‡å­¦ä¹ æƒé‡çš„æ–¹æ³•ï¼Œç¡®å®šäº†æ¯ä¸ªåŸºå­¦ä¹ å™¨å¯¹äºç»“æœçš„è´¡çŒ®å¤§å°ï¼Œä»è€Œæå‡äº†é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚</li>
	<li>Baggingçš„XGBooståŸºå­¦ä¹ å™¨è®­ç»ƒçš„æ—¶å€™å¯¹æ¯ä¸ªåŸºå­¦ä¹ å™¨éšæœºé€‰å–ä¸€éƒ¨åˆ†å±æ€§è¿›è¡Œè®­ç»ƒï¼Œä»è€Œè¾¾åˆ°BaggingåŸºå­¦ä¹ å™¨å¥½è€Œä¸åŒçš„è¦æ±‚ã€‚</li>
	</ul>
    </p>
	<p>å®ç”¨æ€§æ–¹é¢ï¼ŒXGBoost-Baggingå¯ä»¥ç”¨äºæ ·æœ¬æ•°é‡è¾ƒå°‘ï¼Œå¯¹ç²¾åº¦è¦æ±‚è¾ƒé«˜çš„åœºæ™¯ã€‚æœ¬æ¬¡æˆ‘ä½¿ç”¨äº†ç™Œç»†èƒæ•°æ®é›†ï¼Œä½¿ç”¨XGBoost-Baggingæ¥è§£å†³åˆ¤æ–­ç™Œç»†èƒæ˜¯è‰¯æ€§è¿˜æ˜¯æ¶æ€§çš„é—®é¢˜ï¼Œè¯æ˜äº†è¿™ç§å­¦ä¹ å™¨åœ¨åŒ»ç–—è¯Šæ–­æ–¹é¢çš„æ½œåŠ›ã€‚è¿™åœ¨åŒ»ç–—æ–¹é¢çš„å®ç”¨æ€§è¿˜æ˜¯å¾ˆå¼ºçš„ã€‚</p>
    <h2>4. è¿è¡Œæ–¹æ³•å’Œå‚æ•°è®¾ç½®</h2>
    <p>åªè¦è¿è¡Œmain.pyæ–‡ä»¶å°±å¯ä»¥äº†ã€‚éœ€è¦è®¾ç½®çš„å‚æ•°æœ‰ï¼š</p>
	n_estimator=15, attributeRate=0.6, train=True, w_train=True, model_path="../model/40_6_models.pkl", sample_list_path="../model/40_6_sampleLists.pkl", logistic_path="../model/40_6_logistic.pkl"
	<ul>
	<li>n_estimatorï¼šåŸºå­¦ä¹ å™¨çš„æ•°é‡</li>
	<li>attributeRateï¼šè®­ç»ƒåŸºå­¦ä¹ å™¨æ—¶æ¯ä¸ªåŸºå­¦ä¹ å™¨ä½¿ç”¨çš„å±æ€§å æ€»å±æ€§çš„æ¯”ä¾‹</li>
	<li>trainï¼šè®­ç»ƒè¿˜æ˜¯ä½¿ç”¨å·²æœ‰æ¨¡å‹</li>
	<li>train_wï¼šè®­ç»ƒwè¿˜æ˜¯ä½¿ç”¨å·²æœ‰w</li>
	<li>model_pathï¼šå­˜å‚¨åŸºå­¦ä¹ å™¨XGBoostæ¨¡å‹çš„æ–‡ä»¶çš„è·¯å¾„</li>
	<li>sample_list_pathï¼šè¯¥è·¯å¾„æ–‡ä»¶å­˜å‚¨äº†æ¯ä¸ªåŸºå­¦ä¹ å™¨ä½¿ç”¨çš„å±æ€§çš„åˆ—ä¸‹æ ‡ï¼ˆå¯¹äºå†æ¬¡é¢„æµ‹æ—¶è¿˜åŸä¹‹å‰çš„æ¨¡å‹çŠ¶æ€å¾ˆé‡è¦ï¼‰</li>
	<li>logistic_pathï¼šå­˜å‚¨ç”¨äºæŠ•ç¥¨çš„Logistic Regressionçš„æ¨¡å‹è·¯å¾„</li>
	</ul>
	<p>è¿è¡Œç»“æœï¼š</p>
	<p>åœ¨æ•°æ®è¯»å–é˜¶æ®µï¼Œæˆ‘è¿›è¡Œäº†æ•°æ®çš„å¯è§†åŒ–ï¼Œå¯¹æ¯ä¸ªå±æ€§è¿›è¡Œç»˜å›¾ï¼Œå›¾ä¸­çº¢è‰²ç‚¹ä¸ºæ¶æ€§ï¼Œè“è‰²ç‚¹ä¸ºè‰¯æ€§ï¼Œå¦‚ä¸‹å›¾ï¼š</p>
		 <div style="">  <img src="images/attributes.png"  height="600" style="margin: 0 atuo"/></div>
	<p>é€šè¿‡è®­ç»ƒï¼Œå¾—å‡ºçš„loss curveå¦‚ä¸‹å›¾ï¼Œæ˜¾ç¤ºçš„æ˜¯ä¸€ä¸ªBaggingçš„10ä¸ªXGBoostçš„loss curveï¼š</p>
			 <div style="">  <img src="images/loss_curves.png"  height="800" style="margin: 0 atuo"/></div>
	<p>å‡†ç¡®ç‡ï¼š10æ¬¡æµ‹è¯•çš„å¹³å‡å‡†ç¡®ç‡ä¸º0.9157(æ ·æœ¬ä¸­%80ä¸ºè®­ç»ƒé›†ï¼Œ%20ä¸ºæµ‹è¯•é›†ï¼Œæ¯ä¸€æ¬¡è®­ç»ƒé›†å’Œæµ‹è¯•é›†éšæœºé‡‡æ ·)</p>
	<table border="1">
		<tr>
		<th>ç¼–å·</th><th>1</th><th>2</th><th>3</th><th>4</th><th>5</th><th>6</th><th>7</th><th>8</th><th>9</th><th>10</th><th>max</th><th>ave</th>
		</tr>
		<tr>
		<th>å‡†ç¡®ç‡</th><th>0.8994</th><th>0.9182</th><th>0.9497</th><th>0.9245</th><th>0.9057</th><th>0.9182</th><th>0.9496</th><th>0.9056</th><th>0.9182</th><th>0.8679</th><th>0.9497</th><th>0.9157</th>
		</tr>
	</table>
    <h2>5. å­¦ä¹ å¿ƒå¾—å’Œæ”¶è·</h2>
    <p>é€šè¿‡æœ¬å­¦æœŸçš„å­¦ä¹ ï¼Œæˆ‘è¿›ä¸€æ­¥åŠ æ·±äº†å¯¹äºpythonè¯­è¨€çš„ç†è§£ï¼ŒæŒæ¡äº†å¯¹äºpythonè¯­è¨€çš„æ›´å¤šè¿ç”¨æ–¹æ³•ã€‚è™½ç„¶ä¹‹å‰ä¹Ÿæ¥è§¦è¿‡pythonï¼Œä½†æ›´å¤šæ˜¯ç”¨ç¼–å†™cè¯­è¨€çš„æ€ç»´æ¥ç¼–å†™pythonï¼Œç»è¿‡æœ¬è¯¾ç¨‹çš„å­¦ä¹ ï¼Œæˆ‘å¯¹äºpythonè¯­è¨€æœ¬èº«çš„ç‰¹ç‚¹å’Œä¼˜åŠ¿æœ‰äº†æ›´æ·±çš„ç†è§£ã€‚ç»è¿‡è¯¾ç¨‹ä¸­çš„å¤§é‡ç»ƒä¹ ï¼Œæˆ‘ç¼–å†™pythonä»£ç çš„èƒ½åŠ›æœ‰äº†å¾ˆå¤§æå‡ï¼Œèƒ½å¤Ÿè‡ªå·±ç¼–å†™è¾ƒå¤§è§„æ¨¡çš„pythonç¨‹åºã€‚
    </p>
	<p>é€šè¿‡è¯¾ç¨‹çš„å­¦ä¹ å’Œè¯¾ä¸‹å¯¹è¯¾ç¨‹èµ„æ–™çš„ä»”ç»†é˜…è¯»ï¼Œæˆ‘å¯¹äºnumpyï¼Œpandasï¼Œmatplotlibï¼Œscipyç­‰ç§‘å­¦æŠ€æœ¯åº“æœ‰äº†åˆæ­¥çš„æŒæ¡ï¼Œæ„Ÿå—åˆ°äº†pythonä½œä¸ºç§‘å­¦è®¡ç®—è¯­è¨€çš„ä¼˜åŠ¿ï¼Œä¸ºä¸‹ä¸€æ­¥çš„å­¦ä¹ å¯ç§‘ç ”æ‰“ä¸‹äº†åŸºç¡€ã€‚</p>
	<p>è¿™ä¸ªå­¦æœŸçš„pythonè¯¾ç¨‹å¸¦ç»™äº†æˆ‘å¾ˆå¤§çš„æ”¶è·ï¼Œä¹ŸåŸ¹å…»äº†æˆ‘å¯¹äºpythonè¯­è¨€çš„å…´è¶£ï¼Œæ¥ä¸‹æ¥æˆ‘ä¹Ÿä¼šè¿›ä¸€æ­¥æ¢ç´¢pythonè¯­è¨€çš„å¥¥å¦™ã€‚</p>
    <h2>å‚è€ƒèµ„æ–™</h2>
    <p>Chen, Tianqi, et al. "Xgboost: extreme gradient boosting." R package version 0.4-2 1.4 (2015).</p>
	<p>Chen, Tianqi, and Carlos Guestrin. "Xgboost: A scalable tree boosting system." Proceedings of the 22nd acm sigkdd international conference on knowledge discovery and data mining. 2016.</p>
	<p>Breiman, Leo. "Bagging predictors." Machine learning 24.2 (1996).</p>
	<p>BÃ¼hlmann, Peter, and Bin Yu. "Analyzing bagging." The annals of Statistics 30.4 (2002)</p>
	<p>å‘¨å¿—åï¼Œã€Šæœºå™¨å­¦ä¹ ã€‹</p>
	
	
<h2>è®²è§£æ—¥æœŸ</h2>
    <p>12.8è¯¾å ‚ï¼Œå¦‚æœæ’ä¸ä¸Šé¡ºå»¶ã€‚<br>
    </p>	
  </body>
</html>
